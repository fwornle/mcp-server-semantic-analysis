#!/usr/bin/env python3
"""
Enhanced MCP Server for Semantic Analysis
Integrates the 7-agent system: Step 2 - SemanticAnalysisAgent integration
"""

import json
import sys
import os
import asyncio
from typing import Any, Dict, Optional
from abc import ABC, abstractmethod
from pathlib import Path

from mcp.server import Server
from mcp.types import Tool, TextContent
from mcp.server.stdio import stdio_server

# Add the agents directory to the path
sys.path.insert(0, str(Path(__file__).parent / "agents"))
sys.path.insert(0, str(Path(__file__).parent / "config"))

# Enhanced analysis capabilities - embedded from 7-agent system
class APIKeyManager:
    """Manages API keys and provides fallback chain."""
    
    def __init__(self):
        self.providers = {}
        self.detect_available_providers()
    
    def detect_available_providers(self):
        """Detect available API providers."""
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY") 
        openai_base = os.getenv("OPENAI_BASE_URL")
        
        if anthropic_key and anthropic_key != "your-anthropic-api-key":
            self.providers["anthropic"] = True
        if openai_key and openai_key != "your-openai-api-key":
            self.providers["openai"] = True
        if openai_base and openai_key:
            self.providers["custom_openai"] = True
    
    def get_fallback_chain(self):
        """Get the fallback chain in order of preference."""
        chain = []
        if "anthropic" in self.providers:
            chain.append("anthropic")
        if "openai" in self.providers:
            chain.append("openai")
        if "custom_openai" in self.providers:
            chain.append("custom_openai")
        return chain
    
    def get_status_report(self):
        """Get status report of available providers."""
        return {
            "has_ai_providers": len(self.providers) > 0,
            "fallback_chain": self.get_fallback_chain(),
            "providers": self.providers
        }

print("✅ Enhanced API key management available", file=sys.stderr)


class BaseAgent(ABC):
    """
    Base class for all agents in the semantic analysis system.
    Provides common functionality and enforces interface contracts.
    """
    
    def __init__(self, name: str, config: Dict[str, Any], system: Any = None):
        self.name = name
        self.config = config
        self.system = system
        self.running = False
        self.capabilities = []
        
        # Event handling
        self._event_handlers = {}
        
    async def initialize(self):
        """Initialize the agent."""
        try:
            await self.on_initialize()
            self.running = True
            print(f"✅ {self.name} agent initialized successfully", file=sys.stderr)
            
        except Exception as e:
            print(f"❌ Failed to initialize {self.name} agent: {str(e)}", file=sys.stderr)
            raise
    
    @abstractmethod
    async def on_initialize(self):
        """Agent-specific initialization logic."""
        pass
    
    async def shutdown(self):
        """Shutdown the agent gracefully."""
        try:
            await self.on_shutdown()
            self.running = False
            print(f"✅ {self.name} agent shut down successfully", file=sys.stderr)
            
        except Exception as e:
            print(f"❌ Error shutting down {self.name} agent: {str(e)}", file=sys.stderr)
    
    async def on_shutdown(self):
        """Agent-specific shutdown logic."""
        pass
    
    async def health_check(self) -> Dict[str, Any]:
        """Check agent health and return status."""
        return {
            "healthy": self.running,
            "name": self.name,
            "capabilities": self.capabilities
        }
    
    def register_capability(self, capability: str):
        """Register a capability this agent provides."""
        if capability not in self.capabilities:
            self.capabilities.append(capability)
    
    def has_capability(self, capability: str) -> bool:
        """Check if agent has a specific capability."""
        return capability in self.capabilities
    
    async def handle_event(self, event_type: str, data: Dict[str, Any]) -> Optional[Any]:
        """Handle an event sent to this agent."""
        handler = self._event_handlers.get(event_type)
        if handler:
            try:
                return await handler(data)
            except Exception as e:
                print(f"❌ Event handler failed: {event_type} - {str(e)}", file=sys.stderr)
                raise
        else:
            return None
    
    def register_event_handler(self, event_type: str, handler):
        """Register an event handler for a specific event type."""
        self._event_handlers[event_type] = handler


class LLMProvider(BaseAgent):
    """Base class for LLM providers."""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.register_capability("llm_analysis")
    
    async def on_initialize(self):
        """Initialize the LLM provider."""
        # Provider-specific initialization
        pass
        
    async def analyze(self, prompt: str, content: str, options: Dict[str, Any] = None) -> Dict[str, Any]:
        """Analyze content with the LLM."""
        raise NotImplementedError
        
    def validate_config(self) -> bool:
        """Validate provider configuration."""
        raise NotImplementedError


class ClaudeProvider(LLMProvider):
    """Enhanced Claude (Anthropic) LLM provider with sophisticated prompts."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__("claude_provider", config)
        self.api_key = os.getenv("ANTHROPIC_API_KEY")
        
    def validate_config(self) -> bool:
        return bool(self.api_key and self.api_key != "your-anthropic-api-key")
    
    def _build_system_prompt(self, analysis_type: str) -> str:
        """Build enhanced system prompt for Claude."""
        base_prompt = """You are an expert semantic analysis AI specializing in code analysis, technical documentation, and software development patterns.

Your responses should be precise, structured, and focused on actionable insights. Always respond in valid JSON format when requested."""

        type_specific_prompts = {
            "general": "Provide comprehensive analysis with clear structure.",
            "code": "Focus on architectural patterns, design decisions, and technical significance.",
            "conversation": "Extract key decisions, rationales, and transferable insights.",
            "pattern_extraction": "Identify and categorize specific patterns with clear examples.",
            "insight_generation": "Generate actionable insights from raw analysis data.",
            "significance_scoring": "Evaluate technical significance and provide numerical score."
        }

        return f"{base_prompt}\n\n{type_specific_prompts.get(analysis_type, type_specific_prompts['general'])}"
    
    async def analyze(self, prompt: str, content: str, options: Dict[str, Any] = None) -> Dict[str, Any]:
        """Analyze content using Claude API with enhanced prompts."""
        if not self.validate_config():
            return {"success": False, "error": "Claude API key not configured", "provider": "claude"}
        
        options = options or {}
        analysis_type = options.get("analysis_type", "general")
        
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=self.api_key)
            
            system_prompt = self._build_system_prompt(analysis_type)
            user_prompt = f"{prompt}\n\n=== CONTENT TO ANALYZE ===\n{content}"
            
            response = await asyncio.to_thread(
                client.messages.create,
                model=options.get("model", "claude-3-5-sonnet-20241022"),
                max_tokens=options.get("max_tokens", 4096),
                temperature=options.get("temperature", 0.3),
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            result = self._parse_response(response.content[0].text, options)
            
            return {
                "success": True,
                "result": result,
                "provider": "claude",
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "claude"
            }
    
    def _parse_response(self, response_text: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """Parse Claude's response with structured extraction."""
        try:
            # Try to parse as JSON first
            import re
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                return json.loads(json_match.group(0))
            
            # Fall back to structured text parsing
            return {
                "analysis": response_text,
                "structured": self._extract_structured_data(response_text),
                "timestamp": asyncio.get_event_loop().time()
            }
            
        except Exception:
            return {
                "analysis": response_text,
                "structured": None,
                "timestamp": asyncio.get_event_loop().time()
            }
    
    def _extract_structured_data(self, text: str) -> Optional[Dict[str, Any]]:
        """Extract structured data from text response."""
        structured = {}
        
        # Extract patterns like "Key: Value"
        import re
        key_value_regex = re.compile(r'^([A-Z][a-zA-Z\s]+):\s*(.+)$', re.MULTILINE)
        
        for match in key_value_regex.finditer(text):
            key = match.group(1).lower().replace(' ', '_')
            value = match.group(2).strip()
            structured[key] = value
        
        return structured if structured else None


class OpenAIProvider(LLMProvider):
    """Enhanced OpenAI LLM provider with 3-tier fallback and sophisticated prompts."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__("openai_provider", config)
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_BASE_URL")
        
    def validate_config(self) -> bool:
        return bool(self.api_key and self.api_key != "your-openai-api-key")
    
    def _build_system_prompt(self, analysis_type: str) -> str:
        """Build enhanced system prompt for OpenAI."""
        base_prompt = """You are an expert semantic analysis AI specializing in code analysis, technical documentation, and software development patterns.

Your responses should be precise, structured, and focused on actionable insights. Always respond in valid JSON format when requested."""

        type_specific_prompts = {
            "general": "Provide comprehensive analysis with clear structure.",
            "code": "Focus on architectural patterns, design decisions, and technical significance.",
            "conversation": "Extract key decisions, rationales, and transferable insights.",
            "pattern_extraction": "Identify and categorize specific patterns with clear examples.",
            "insight_generation": "Generate actionable insights from raw analysis data.",
            "significance_scoring": "Evaluate technical significance and provide numerical score."
        }

        return f"{base_prompt}\n\n{type_specific_prompts.get(analysis_type, type_specific_prompts['general'])}"
    
    async def analyze(self, prompt: str, content: str, options: Dict[str, Any] = None) -> Dict[str, Any]:
        """Analyze content using OpenAI API with enhanced prompts."""
        if not self.validate_config():
            return {"success": False, "error": "OpenAI API key not configured", "provider": "openai"}
        
        options = options or {}
        analysis_type = options.get("analysis_type", "general")
        
        try:
            import openai
            
            client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            system_prompt = self._build_system_prompt(analysis_type)
            user_prompt = f"{prompt}\n\n=== CONTENT TO ANALYZE ===\n{content}"
            
            # Determine model based on whether we're using custom OpenAI
            model = "gpt-4"
            if self.base_url:
                # Custom OpenAI endpoint - might be using different models
                model = options.get("model", "gpt-4")
            
            response = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=options.get("temperature", 0.3),
                max_tokens=options.get("max_tokens", 4096)
            )
            
            result = self._parse_response(response.choices[0].message.content, options)
            
            provider_name = "custom_openai" if self.base_url else "openai"
            
            return {
                "success": True,
                "result": result,
                "provider": provider_name,
                "usage": {
                    "input_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "output_tokens": response.usage.completion_tokens if response.usage else 0
                }
            }
            
        except Exception as e:
            provider_name = "custom_openai" if self.base_url else "openai"
            return {
                "success": False,
                "error": str(e),
                "provider": provider_name
            }
    
    def _parse_response(self, response_text: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """Parse OpenAI's response with structured extraction."""
        try:
            # Try to parse as JSON first
            import re
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                return json.loads(json_match.group(0))
            
            # Fall back to structured text parsing
            return {
                "analysis": response_text,
                "structured": self._extract_structured_data(response_text),
                "timestamp": asyncio.get_event_loop().time()
            }
            
        except Exception:
            return {
                "analysis": response_text,
                "structured": None,
                "timestamp": asyncio.get_event_loop().time()
            }
    
    def _extract_structured_data(self, text: str) -> Optional[Dict[str, Any]]:
        """Extract structured data from text response."""
        structured = {}
        
        # Extract patterns like "Key: Value"
        import re
        key_value_regex = re.compile(r'^([A-Z][a-zA-Z\s]+):\s*(.+)$', re.MULTILINE)
        
        for match in key_value_regex.finditer(text):
            key = match.group(1).lower().replace(' ', '_')
            value = match.group(2).strip()
            structured[key] = value
        
        return structured if structured else None


class SemanticAnalysisEngine(BaseAgent):
    """Enhanced semantic analysis engine with 3-tier fallback system."""
    
    def __init__(self):
        super().__init__("semantic_analysis_engine", {})
        self.providers = {}
        self.api_key_manager = APIKeyManager()
        self.primary_provider = None
        self.fallback_provider = None
        self.analysis_cache = {}
        
        self.register_capability("semantic_analysis")
        self.register_capability("3_tier_fallback")
        self.register_capability("enhanced_prompts")
        self.register_capability("response_parsing")
        self.register_capability("caching")
    
    async def on_initialize(self):
        """Initialize the semantic analysis engine with 3-tier fallback."""
        print("🔄 Initializing enhanced semantic analysis engine with 3-tier fallback...", file=sys.stderr)
        await self._initialize_providers()
        
        status = self.api_key_manager.get_status_report()
        print(f"✅ API Status: {status['has_ai_providers']} providers, chain: {status['fallback_chain']}", file=sys.stderr)
        
        if not status["has_ai_providers"]:
            print("⚠️  No AI providers available - using UKB-CLI fallback mode", file=sys.stderr)
    
    async def _initialize_providers(self):
        """Initialize LLM providers in 3-tier fallback order."""
        fallback_chain = self.api_key_manager.get_fallback_chain()
        
        for provider_type in fallback_chain:
            try:
                if provider_type == "anthropic":
                    provider = ClaudeProvider({})
                elif provider_type in ("openai", "custom_openai"):
                    provider = OpenAIProvider({})
                else:
                    continue
                
                if provider.validate_config():
                    await provider.initialize()
                    self.providers[provider_type] = provider
                    
                    if not self.primary_provider:
                        self.primary_provider = provider
                        print(f"✅ Primary provider: {provider_type}", file=sys.stderr)
                    elif not self.fallback_provider:
                        self.fallback_provider = provider
                        print(f"✅ Fallback provider: {provider_type}", file=sys.stderr)
                
            except Exception as e:
                print(f"⚠️  Failed to initialize {provider_type} provider: {e}", file=sys.stderr)
    
    async def analyze_with_llm(self, prompt: str, content: str, analysis_type: str = "general") -> Dict[str, Any]:
        """Analyze content with enhanced 3-tier fallback system."""
        options = {"analysis_type": analysis_type}
        
        # Check cache first
        cache_key = f"{analysis_type}:{hash(content)}:{hash(prompt)}"
        if cache_key in self.analysis_cache:
            cache_entry = self.analysis_cache[cache_key]
            if asyncio.get_event_loop().time() - cache_entry["timestamp"] < 300:  # 5 min TTL
                print("📋 Returning cached analysis result", file=sys.stderr)
                return cache_entry["result"]
        
        # Try primary provider first
        if self.primary_provider:
            try:
                print(f"🔄 Attempting analysis with primary provider ({self.primary_provider.name})", file=sys.stderr)
                result = await self.primary_provider.analyze(prompt, content, options)
                
                if result.get("success"):
                    # Cache successful result
                    self.analysis_cache[cache_key] = {
                        "result": result,
                        "timestamp": asyncio.get_event_loop().time()
                    }
                    print(f"✅ Analysis successful with {result.get('provider', 'unknown')}", file=sys.stderr)
                    return result
                else:
                    print(f"⚠️  Primary provider failed: {result.get('error')}", file=sys.stderr)
                    
            except Exception as e:
                print(f"⚠️  Primary provider exception: {e}", file=sys.stderr)
        
        # Try fallback provider
        if self.fallback_provider:
            try:
                print(f"🔄 Using fallback provider ({self.fallback_provider.name})", file=sys.stderr)
                result = await self.fallback_provider.analyze(prompt, content, options)
                
                if result.get("success"):
                    # Cache successful result
                    self.analysis_cache[cache_key] = {
                        "result": result,
                        "timestamp": asyncio.get_event_loop().time()
                    }
                    print(f"✅ Analysis successful with fallback {result.get('provider', 'unknown')}", file=sys.stderr)
                    return result
                else:
                    print(f"⚠️  Fallback provider failed: {result.get('error')}", file=sys.stderr)
                    
            except Exception as e:
                print(f"⚠️  Fallback provider exception: {e}", file=sys.stderr)
        
        # Final fallback - UKB-CLI mode
        print("🔄 Using UKB-CLI fallback mode", file=sys.stderr)
        return await self._ukb_fallback_analysis(prompt, content, analysis_type)
    
    async def _ukb_fallback_analysis(self, prompt: str, content: str, analysis_type: str) -> Dict[str, Any]:
        """Final fallback using UKB-CLI for analysis."""
        try:
            # Simple pattern-based analysis when no AI providers available
            word_count = len(content.split())
            char_count = len(content)
            
            # Basic complexity scoring
            complexity_score = min(10, max(1, word_count // 100))
            
            analysis = {
                "analysis_type": analysis_type,
                "word_count": word_count,
                "character_count": char_count,
                "complexity_score": complexity_score,
                "summary": f"Content analysis: {word_count} words, {char_count} characters",
                "provider_note": "Analysis performed using UKB-CLI fallback mode (no AI providers available)"
            }
            
            return {
                "success": True,
                "result": analysis,
                "provider": "ukb_fallback"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"UKB fallback failed: {str(e)}",
                "provider": "ukb_fallback"
            }


async def main():
    """Main entry point for the MCP server."""
    
    # Create the server and semantic analysis engine
    server = Server("semantic-analysis")
    analysis_engine = SemanticAnalysisEngine()
    
    # Initialize the analysis engine (BaseAgent pattern)
    await analysis_engine.initialize()
    
    @server.list_tools()
    async def list_tools() -> list[Tool]:
        """List available tools."""
        return [
            Tool(
                name="test_connection",
                description="Test the connection to the semantic analysis server",
                inputSchema={
                    "type": "object",
                    "properties": {},
                    "additionalProperties": False
                }
            ),
            Tool(
                name="analyze_code", 
                description="Analyze code for patterns and issues",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "code": {
                            "type": "string",
                            "description": "The code to analyze"
                        },
                        "language": {
                            "type": "string", 
                            "description": "Programming language"
                        }
                    },
                    "required": ["code"],
                    "additionalProperties": False
                }
            ),
            Tool(
                name="determine_insights",
                description="Determine insights from analysis results",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "context": {
                            "type": "string",
                            "description": "Context for insight generation"
                        }
                    },
                    "required": ["context"],
                    "additionalProperties": False
                }
            )
        ]
    
    @server.call_tool()
    async def call_tool(name: str, arguments: Dict[str, Any]) -> list[TextContent]:
        """Handle tool calls."""
        
        if name == "test_connection":
            return [TextContent(
                type="text", 
                text="✅ Semantic analysis server connection successful!"
            )]
        
        elif name == "analyze_code":
            code = arguments.get("code", "")
            language = arguments.get("language", "unknown")
            
            # Enhanced LLM-powered analysis
            prompt = f"Analyze this {language} code for patterns, issues, and architectural insights:"
            
            try:
                llm_result = await analysis_engine.analyze_with_llm(prompt, code, "code")
                
                analysis = {
                    "language": language,
                    "lines": len(code.split('\n')),
                    "characters": len(code),
                    "llm_analysis": llm_result.get("result", "No LLM analysis available"),
                    "provider_used": llm_result.get("provider", "unknown"),
                    "status": "analyzed"
                }
                
                return [TextContent(
                    type="text",
                    text=f"Enhanced code analysis complete:\n{json.dumps(analysis, indent=2)}"
                )]
                
            except Exception as e:
                # Fallback to simple analysis
                analysis = {
                    "language": language,
                    "lines": len(code.split('\n')),
                    "characters": len(code),
                    "error": str(e),
                    "status": "fallback_analysis"
                }
                
                return [TextContent(
                    type="text",
                    text=f"Code analysis (fallback):\n{json.dumps(analysis, indent=2)}"
                )]
        
        elif name == "determine_insights":
            context = arguments.get("context", "")
            
            # Enhanced LLM-powered insight generation
            prompt = "Generate actionable insights from this analysis context:"
            
            try:
                llm_result = await analysis_engine.analyze_with_llm(prompt, context, "insight_generation")
                
                insight = {
                    "context": context,
                    "llm_insights": llm_result.get("result", "No LLM insights available"),
                    "provider_used": llm_result.get("provider", "unknown"),
                    "confidence": 0.95,
                    "status": "complete"
                }
                
                return [TextContent(
                    type="text", 
                    text=f"Enhanced insights determined:\n{json.dumps(insight, indent=2)}"
                )]
                
            except Exception as e:
                # Fallback to simple insight
                insight = {
                    "context": context,
                    "insight": "Enhanced MCP server with LLM integration",
                    "error": str(e),
                    "confidence": 0.5,
                    "status": "fallback"
                }
                
                return [TextContent(
                    type="text", 
                    text=f"Insights (fallback):\n{json.dumps(insight, indent=2)}"
                )]
        
        else:
            return [TextContent(
                type="text",
                text=f"Unknown tool: {name}"
            )]
    
    # Run the server with proper async context
    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream, 
            write_stream, 
            server.create_initialization_options()
        )


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())